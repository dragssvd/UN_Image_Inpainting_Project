{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "iwdziNX1hEgq",
        "sKvBm_MmoBZ2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dragssvd/UN_Image_Inpainting_Project/blob/main/ENC_DEC_CLASS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "iwdziNX1hEgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install comet-ml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK9jlc5Mjj_6",
        "outputId": "5bb9cbab-3409-456a-bf9d-07433b08e6e0"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: comet-ml in /usr/local/lib/python3.10/dist-packages (3.47.2)\n",
            "Requirement already satisfied: everett<3.2.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from everett[ini]<3.2.0,>=1.0.1->comet-ml) (3.1.0)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (4.23.0)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (5.9.5)\n",
            "Requirement already satisfied: python-box<7.0.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (6.1.0)\n",
            "Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (2.32.3)\n",
            "Requirement already satisfied: semantic-version>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (2.10.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (2.18.0)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.10/dist-packages (from comet-ml) (3.19.3)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (2.2.3)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (1.16.0)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (3.1.1)\n",
            "Requirement already satisfied: dulwich!=0.20.33,>=0.20.6 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (0.22.5)\n",
            "Requirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (13.9.4)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.10/dist-packages (from everett[ini]<3.2.0,>=1.0.1->comet-ml) (5.0.9)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet-ml) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet-ml) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet-ml) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet-ml) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet-ml) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet-ml) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet-ml) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "4vOcI797gS8w"
      },
      "outputs": [],
      "source": [
        "import comet_ml\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import einops\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CometML"
      ],
      "metadata": {
        "id": "aRjw5z-ain2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "key = userdata.get('COMET_API_KEY')\n",
        "os.environ['COMET_API_KEY'] = key"
      ],
      "metadata": {
        "id": "8OTpA4h2ishT"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pick device"
      ],
      "metadata": {
        "id": "sKvBm_MmoBZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        # Set default tensor type for cuda\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device('mps')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        # Ensure we're using float32 on CPU\n",
        "        torch.set_default_dtype(torch.float64)\n",
        "    return device\n",
        "\n",
        "device = setup_device()\n",
        "\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFFyecY7gYqC",
        "outputId": "35871fb5-b4c1-46e8-de77-c79393dfc104"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e1XoJTWbgnEl"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "fWDzQYsxmOYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pYpkIaqmWZc",
        "outputId": "45f5c09d-464d-474f-d45f-511617330d11"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "ds = load_dataset(\"Artificio/WikiArt_Full\").with_format(\"torch\")"
      ],
      "metadata": {
        "id": "bn3ODiYsmSdb"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrxZQGBPnAaj",
        "outputId": "b6df17a9-4453-4ba5-ccf1-e1c2bd610621"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'embeddings_pca512', 'resnet50_non_robust_feats', 'resnet50_robust_feats'],\n",
              "        num_rows: 103250\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 90% train, 10% test + validation\n",
        "train_testvalid = ds['train'].train_test_split(test_size=0.2)\n",
        "# Split the 10% test + valid in half test, half valid\n",
        "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
        "# gather everyone if you want to have a single DatasetDict\n",
        "train_test_valid_dataset = DatasetDict({\n",
        "    'train': train_testvalid['train'],\n",
        "    'test': test_valid['test'],\n",
        "    'valid': test_valid['train']}).with_format(\"torch\")\n",
        "\n",
        "ds = train_test_valid_dataset\n",
        "print(ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyWVP91nnZA_",
        "outputId": "ac7a9601-d089-4765-de00-35f6c994d15f"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'embeddings_pca512', 'resnet50_non_robust_feats', 'resnet50_robust_feats'],\n",
            "        num_rows: 82600\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'embeddings_pca512', 'resnet50_non_robust_feats', 'resnet50_robust_feats'],\n",
            "        num_rows: 10325\n",
            "    })\n",
            "    valid: Dataset({\n",
            "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'embeddings_pca512', 'resnet50_non_robust_feats', 'resnet50_robust_feats'],\n",
            "        num_rows: 10325\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Extract 10% of the train set\n",
        "# ten_percent_train = ds[\"train\"].select(range(int(len(ds[\"train\"]) * 0.1)))\n",
        "\n",
        "# # Extract 10% of the test set\n",
        "# ten_percent_test = ds[\"test\"].select(range(int(len(ds[\"test\"]) * 0.1)))\n",
        "\n",
        "# # Extract 10% of the validation set\n",
        "# ten_percent_valid = ds[\"valid\"].select(range(int(len(ds[\"valid\"]) * 0.1)))\n",
        "\n",
        "# # Combine the subsets into a new DatasetDict\n",
        "# ten_percent_dataset = DatasetDict({\n",
        "#     \"train\": ten_percent_train,\n",
        "#     \"test\": ten_percent_test,\n",
        "#     \"valid\": ten_percent_valid\n",
        "# })\n",
        "\n",
        "# ds = ten_percent_dataset\n",
        "# # for testing"
      ],
      "metadata": {
        "id": "oAGj36JmyMJW"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ds[\"test\"][\"image\"][42]"
      ],
      "metadata": {
        "id": "xah5ORG0nByy"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ds[\"test\"][\"style\"][42]"
      ],
      "metadata": {
        "id": "CiQasMZJ_0BC"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Assuming 'ds' is your DatasetDict\n",
        "# image = ds[\"test\"][\"image\"][42]\n",
        "# # Convert PIL.Image.Image to NumPy array\n",
        "# image_np = np.array(image)\n",
        "# # Transpose the dimensions\n",
        "# plt.imshow(image_np.transpose(1, 2, 0))\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "3ZSodJZeAL5I"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader\n"
      ],
      "metadata": {
        "id": "Z806R5YXo2_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # change if not colab\n",
        "  num_workers = 2\n",
        "  pin_memory = True\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "      ds[\"train\"],\n",
        "      batch_size=32,\n",
        "      num_workers=num_workers,\n",
        "      shuffle=True,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "\n",
        "  test_loader = DataLoader(\n",
        "      ds[\"test\"],\n",
        "      batch_size=32,\n",
        "      num_workers=num_workers,\n",
        "      shuffle=False,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "\n",
        "  val_loader = DataLoader(\n",
        "      ds[\"test\"],\n",
        "      batch_size=32,\n",
        "      num_workers=num_workers,\n",
        "      shuffle=False,\n",
        "      pin_memory=True,\n",
        "  )"
      ],
      "metadata": {
        "id": "0Ymf-sKto2sK"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder\n"
      ],
      "metadata": {
        "id": "EiXUEIKthJ1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Encoders**"
      ],
      "metadata": {
        "id": "64___TlvlWBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic encoder"
      ],
      "metadata": {
        "id": "guemmKjGtJhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import einops\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    @staticmethod\n",
        "    def conv_block(in_size: int, out_size: int):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_size, out_size, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_size),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def __init__(self, latent_width: int = 2048, **kwargs) -> None:\n",
        "        super().__init__()\n",
        "        self.latent_width = latent_width\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            Encoder.conv_block(3, 64),   # [32, 8, 128, 128]\n",
        "            Encoder.conv_block(64, 128),   # [32, 8, 64, 64]\n",
        "            Encoder.conv_block(128, 256),  # [32, 16, 32, 32]\n",
        "            Encoder.conv_block(256, 512), # [32, 16, 16, 16]\n",
        "            Encoder.conv_block(512, 512), # [32, 16, 8, 8]\n",
        "            Encoder.conv_block(512, 512), # [32, 16, 4, 4]\n",
        "        )\n",
        "\n",
        "        # Adjust Linear layer input to match 16 * 2 * 2\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512 * 4 * 4, self.latent_width),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.model(x)  # Convolutional layers\n",
        "        x = einops.rearrange(x, \"b c w h -> b (c w h)\")  # Flatten for Linear\n",
        "        x = self.fc(x)  # Fully connected layers\n",
        "        return x"
      ],
      "metadata": {
        "id": "yaGE672jhLHv"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variance Encoder"
      ],
      "metadata": {
        "id": "sLGp5gSgtMAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VarianceEncoder(nn.Module):\n",
        "    @staticmethod\n",
        "    def conv_block(in_size: int, out_size: int):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_size, out_size, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_size),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "\n",
        "    def __init__(self, latent_width: int = 2048, **kwargs) -> None:\n",
        "        super().__init__()\n",
        "        self.latent_width = latent_width\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            Encoder.conv_block(3, 64),   # [32, 8, 128, 128]\n",
        "            Encoder.conv_block(64, 128),   # [32, 8, 64, 64]\n",
        "            Encoder.conv_block(128, 256),  # [32, 16, 32, 32]\n",
        "            Encoder.conv_block(256, 512), # [32, 16, 16, 16]\n",
        "            Encoder.conv_block(512, 512), # [32, 16, 8, 8]\n",
        "            Encoder.conv_block(512, 512), # [32, 16, 4, 4]\n",
        "        )\n",
        "\n",
        "        self.fc_mu = nn.Linear(512 * 4 * 4, self.latent_width)\n",
        "        self.fc_logvar = nn.Linear(512 * 4 * 4, self.latent_width)\n",
        "\n",
        "    def reparametrize(self, mean: torch.Tensor, logvariance: torch.Tensor):\n",
        "        # transform N(0,1) into N(mean, variance)\n",
        "        std_dev = torch.exp(0.5 * logvariance)\n",
        "        eps = torch.randn_like(std_dev)\n",
        "        return eps * std_dev + mean\n",
        "\n",
        "    def encode(self, x: torch.Tensor):\n",
        "        x = self.model(x)\n",
        "        x = einops.rearrange(x, \"b c w h -> b (c w h)\")\n",
        "        mean = self.fc_mu(x)\n",
        "        logvariance = self.fc_logvar(x)\n",
        "        return mean, logvariance\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        mean, logvariance = self.encode(x)\n",
        "        z = self.reparametrize(mean, logvariance)\n",
        "        return z, mean, logvariance"
      ],
      "metadata": {
        "id": "x_bOxj9mtRMq"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss function for variance encoder"
      ],
      "metadata": {
        "id": "Er3TCMeOthwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kld_loss_func(mean, logvariance):\n",
        "    # distance from normal distribution\n",
        "\n",
        "    # commonly seen version:\n",
        "    # return torch.mean(\n",
        "    #     -0.5 * torch.sum(1 + logvariance - mean**2 - logvariance.exp(), dim=1),\n",
        "    #     dim=0,\n",
        "    # )\n",
        "\n",
        "    loss = einops.reduce(\n",
        "        1 + logvariance - mean**2 - torch.exp(logvariance),\n",
        "        \"batch latent -> batch\",\n",
        "        \"sum\",\n",
        "    )\n",
        "    return einops.reduce(-0.5 * loss, \"batch -> ()\", \"mean\")\n"
      ],
      "metadata": {
        "id": "Ip5Xo0mJthDS"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "FPs3y8rtlYhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import einops\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    @staticmethod\n",
        "    def conv_block(in_size: int, out_size: int):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_size,\n",
        "                out_size,\n",
        "                kernel_size=3,\n",
        "                stride=2,\n",
        "                padding=1,\n",
        "                output_padding=1,  # Ensures alignment of dimensions\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_size),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def __init__(self, latent_width: int = 2048, **kwargs) -> None:\n",
        "        super().__init__()\n",
        "        self.latent_width = latent_width\n",
        "\n",
        "        # Fully connected layer to project latent vector into feature map\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.latent_width, 512 * 4 * 4),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Transposed convolutions to upsample\n",
        "        self.model = nn.Sequential(\n",
        "            Decoder.conv_block(512, 512), # [32, 16, 8, 8]\n",
        "            Decoder.conv_block(512, 512), # [32, 16, 4, 4]\n",
        "            Decoder.conv_block(512, 256), # [32, 16, 16, 16]\n",
        "            Decoder.conv_block(256, 128),  # [32, 16, 32, 32]\n",
        "            Decoder.conv_block(128, 64),   # [32, 8, 64, 64]\n",
        "            Decoder.conv_block(64, 3),   # [32, 8, 128, 128]\n",
        "        )\n",
        "\n",
        "        # Final activation ensures output in [0, 1] range\n",
        "        self.final_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # Fully connected layer to expand latent vector\n",
        "        x = self.fc(x)\n",
        "        x = einops.rearrange(x, \"b (c w h) -> b c w h\", c=512, w=4, h=4)\n",
        "\n",
        "        # Transposed convolutions to reconstruct image\n",
        "        x = self.model(x)\n",
        "\n",
        "        # Final activation for output normalization\n",
        "        x = self.final_activation(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ugzyL3hllZZ-"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoencoder"
      ],
      "metadata": {
        "id": "p8oUs4Xkldu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder: VarianceEncoder,\n",
        "        decoder: Decoder,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, images: torch.Tensor):\n",
        "        latent = self.encoder(images)\n",
        "        decoded = self.decoder(latent)\n",
        "\n",
        "        return decoded, latent"
      ],
      "metadata": {
        "id": "Xpf1E7e4lfrD"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VarianceAutoencoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder: VarianceEncoder,\n",
        "        decoder: Decoder,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, images: torch.Tensor):\n",
        "        latent, mean, logvariance = self.encoder(images)\n",
        "        decoded = self.decoder(latent)\n",
        "\n",
        "        return decoded, latent, mean, logvariance\n"
      ],
      "metadata": {
        "id": "ZmAuDrzmoY0j"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model setup"
      ],
      "metadata": {
        "id": "G1mI4jQIpaFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "reconstruction_loss_func = F.mse_loss\n",
        "latent_width = 2048\n",
        "encoder = VarianceEncoder(latent_width)\n",
        "decoder = Decoder(encoder.latent_width)\n",
        "model = VarianceAutoencoder(encoder, decoder)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PN1c-nIRpd1g",
        "outputId": "be6ecd9e-afe9-47e9-b983-42a9c268f4de"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VarianceAutoencoder(\n",
              "  (encoder): VarianceEncoder(\n",
              "    (model): Sequential(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "      (4): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "      (5): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (fc_mu): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "    (fc_logvar): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (fc): Sequential(\n",
              "      (0): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "      (1): ReLU()\n",
              "    )\n",
              "    (model): Sequential(\n",
              "      (0): Sequential(\n",
              "        (0): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "      (4): Sequential(\n",
              "        (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "      (5): Sequential(\n",
              "        (0): ConvTranspose2d(64, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "        (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (final_activation): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comet_ML setup logger"
      ],
      "metadata": {
        "id": "hdhIYIfTpnya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krULQnuisYL3",
        "outputId": "21dca97f-f5b0-46e4-8a44-fc1f74c6d502"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.1+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging\n",
        "\n",
        "from torch.utils.data import DataLoader  # NOQA\n",
        "from datasets import load_dataset  # NOQA\n",
        "\n",
        "from torchinfo import summary  # NOQA\n",
        "\n",
        "from tqdm import tqdm  # NOQA\n",
        "\n",
        "from comet_ml.integration.pytorch import log_model  # NOQA\n",
        "\n",
        "from torchmetrics.functional.image import structural_similarity_index_measure  # NOQA\n",
        "\n",
        "comet_experiment = comet_ml.Experiment(\n",
        "    project_name=\"UczenieNienadzorowane\")\n",
        "comet_experiment.log_code(folder=\"/UN\")\n",
        "comet_experiment.log_parameters(\n",
        "    {\n",
        "        \"batch_size\": train_loader.batch_size,\n",
        "        \"train_size\": ds[\"train\"].num_rows,\n",
        "        \"val_size\": ds[\"test\"].num_rows,\n",
        "    }\n",
        ")\n",
        "\n",
        "summ = summary(model, (1, 3, 256, 256), device=device, depth=5)\n",
        "comet_experiment.set_model_graph(f\"{model.__repr__()}\\n{summ}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTfzbggPpne0",
        "outputId": "f15fe0b5-5ae4-4189-bfe8-c163f8751f68"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : varying_kiwi_7901\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/dragssvd/uczenienienadzorowane/8a14a753ec0f4abc8c304e12c0b95f17\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook_url : https://colab.research.google.com/notebook#fileId=1g71kF4UKRRmwHiZDagxH-Z3F2Oa9my65\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size : 32\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss_func  : kld_loss_func\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_epochs : 5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_size : 82600\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_size   : 10325\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/dragssvd/uczenienienadzorowane/f4286d5d5312478aa81fbb869838af50\n",
            "\n",
            "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m We failed to read directory '/UN' for uploading.\n",
            "Please double-check the file path, permissions, and that it is a directory.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/content' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train autoencoder"
      ],
      "metadata": {
        "id": "LfxB9FzCp9tR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pure PyTorch loop\n",
        "num_epochs = 5\n",
        "# loss_func = F.mse_loss\n",
        "loss_func = kld_loss_func\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "RIl_gNpNp34n"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comet_experiment.log_parameter(\"num_epochs\", num_epochs)\n",
        "comet_experiment.log_parameter(\"loss_func\", loss_func.__name__)\n",
        "\n",
        "comet_experiment.add_tag(f\"LOSS: {loss_func.__name__}\")\n"
      ],
      "metadata": {
        "id": "CRpLFMv2qImU"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train and validate\n",
        "for epoch in range(num_epochs):\n",
        "        comet_experiment.set_epoch(epoch)\n",
        "\n",
        "        model.eval()\n",
        "        with comet_experiment.validate() as validat, torch.no_grad() as nograd:\n",
        "            for idx, batch in tqdm(enumerate(val_loader), desc=f\"VAL_{epoch}\"):\n",
        "                comet_experiment.set_step(idx + epoch * len(val_loader))\n",
        "\n",
        "                images = batch[\"image\"] / 255.0\n",
        "                # labels = batch[\"label\"]\n",
        "                images = images.to(device)\n",
        "                predictions, latents = model(images)\n",
        "                loss = loss_func(predictions, images)\n",
        "\n",
        "                metric = structural_similarity_index_measure(\n",
        "                    predictions, images)\n",
        "\n",
        "                comet_experiment.log_metric(\"loss\", loss.item())\n",
        "                comet_experiment.log_metric(\"SSIM\", metric.item())\n",
        "\n",
        "                # if idx < 4:\n",
        "                #     images = einops.rearrange(\n",
        "                #         [images, predictions],\n",
        "                #         \"source batch 1 height width -> batch height (source width)\",\n",
        "                #     ).cpu()\n",
        "                #     comet_experiment.log_image(\n",
        "                #         images[0], f\"images_{idx}\", step=epoch)\n",
        "\n",
        "        model.train()\n",
        "        with comet_experiment.train() as train:\n",
        "            for idx, batch in tqdm(enumerate(train_loader), desc=f\"TRAIN_{epoch}\"):\n",
        "                comet_experiment.set_step(idx + epoch * len(train_loader))\n",
        "\n",
        "                # look at: model.encoder.parameters().__next__()\n",
        "                # look at: model.encoder.parameters().__next__().grad\n",
        "                # look at: latents.shape\n",
        "\n",
        "                optimizer.zero_grad()  # MUST be called on every batch\n",
        "                images = batch[\"image\"] / 255.0\n",
        "                labels = batch[\"style\"]\n",
        "                images = images.cuda()\n",
        "                predictions, latents = model(images)\n",
        "                loss = loss_func(predictions, images)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                metric = structural_similarity_index_measure(\n",
        "                    predictions, images)\n",
        "\n",
        "                comet_experiment.log_metric(\"loss\", loss.item())\n",
        "                comet_experiment.log_metric(\"SSIM\", metric.item())\n",
        "\n",
        "                if not idx % 50:\n",
        "                    comet_experiment.log_histogram_3d(\n",
        "                        latents.detach().cpu(), \"latents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "4fZ3ypPvqJMT",
        "outputId": "f86c6710-4817-4c49-d195-20a21d21ec7b"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "VAL_0: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "EinopsError",
          "evalue": " Error while processing sum-reduction pattern \"batch latent -> batch\".\n Input tensor shape: torch.Size([32, 3, 256, 256]). Additional info: {}.\n Wrong shape: expected 2 dims. Received 4-dim tensor.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/einops/einops.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0mrecipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_transformation_recipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         return _apply_recipe(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/einops/einops.py\u001b[0m in \u001b[0;36m_prepare_transformation_recipe\u001b[0;34m(pattern, operation, axes_names, ndim)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomposition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEinopsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Wrong shape: expected {len(left.composition)} dims. Received {ndim}-dim tensor.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0mleft_composition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEinopsError\u001b[0m: Wrong shape: expected 2 dims. Received 4-dim tensor.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-187-25e6e64875cb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 metric = structural_similarity_index_measure(\n",
            "\u001b[0;32m<ipython-input-179-f6d174586da6>\u001b[0m in \u001b[0;36mkld_loss_func\u001b[0;34m(mean, logvariance)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     loss = einops.reduce(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlogvariance\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogvariance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m\"batch latent -> batch\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/einops/einops.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n Input is list. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Additional info: {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mEinopsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEinopsError\u001b[0m:  Error while processing sum-reduction pattern \"batch latent -> batch\".\n Input tensor shape: torch.Size([32, 3, 256, 256]). Additional info: {}.\n Wrong shape: expected 2 dims. Received 4-dim tensor."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "model.eval()\n",
        "with comet_experiment.test() as test, torch.no_grad():\n",
        "    for idx, batch in tqdm(enumerate(test_loader), desc=f\"TEST_{num_epochs}\"):\n",
        "        comet_experiment.set_step(idx + num_epochs * len(test_loader))\n",
        "\n",
        "        images = batch[\"image\"] / 255.0\n",
        "        labels = batch[\"style\"]\n",
        "        images = images.cuda()\n",
        "\n",
        "        # Model predictions and latents\n",
        "        predictions, latents = model(images)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_func(predictions, images)\n",
        "\n",
        "        # Compute SSIM metric\n",
        "        metric = structural_similarity_index_measure(predictions, images)\n",
        "\n",
        "        # Log metrics\n",
        "        comet_experiment.log_metric(\"loss\", loss.item())\n",
        "        comet_experiment.log_metric(\"SSIM\", metric.item())\n",
        "\n",
        "        if idx < 4:\n",
        "          try:\n",
        "              # Rearrange tensors to HWC (Height-Width-Channel) format\n",
        "              images = einops.rearrange(\n",
        "                  images, \"batch channel width height -> batch width height channel\"\n",
        "              ).cpu().detach().numpy()\n",
        "\n",
        "              predictions = einops.rearrange(\n",
        "                  predictions, \"batch channel width height -> batch width height channel\"\n",
        "              ).cpu().detach().numpy()\n",
        "\n",
        "              # Ensure pixel values are in range [0, 1] and convert to uint8\n",
        "              images = np.clip(images, 0, 1)\n",
        "              predictions = np.clip(predictions, 0, 1)\n",
        "\n",
        "              images = (images * 255).astype(np.uint8)\n",
        "              predictions = (predictions * 255).astype(np.uint8)\n",
        "\n",
        "              # Log the first image in the batch as an example\n",
        "              combined_image = np.hstack((images[0], predictions[0]))  # Side-by-side\n",
        "              comet_experiment.log_image(combined_image, name=f\"comparison_{idx}\", step=num_epochs)\n",
        "\n",
        "          except Exception as e:\n",
        "              print(f\"Error during image logging: {e}\")\n",
        "              print(f\"Tensor shape: {images.shape if 'images' in locals() else 'N/A'}\")"
      ],
      "metadata": {
        "id": "x-oBjc4BqWyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, batch in tqdm(enumerate(test_loader), desc=f\"TEST_{num_epochs}\"):\n",
        "  images = batch[\"image\"] / 255.0\n",
        "  labels = batch[\"style\"]\n",
        "  images = images.cuda()\n",
        "\n",
        "  # Model predictions and latents\n",
        "  predictions, latents = model(images)\n",
        "\n",
        "  images = (images * 255).byte()\n",
        "  predictions = (predictions * 255).byte()\n",
        "\n",
        "\n",
        "  for i in range(32):\n",
        "    # Convert PIL.Image.Image to NumPy array\n",
        "    image_np = np.array(images[i].to('cpu'))\n",
        "    # Transpose the dimensions\n",
        "    plt.imshow(image_np.transpose(1, 2, 0))\n",
        "    plt.show()\n",
        "\n",
        "      # Convert PIL.Image.Image to NumPy array\n",
        "    image_np = np.array(predictions[i].to('cpu'))\n",
        "    # Transpose the dimensions\n",
        "    plt.imshow(image_np.transpose(1, 2, 0))\n",
        "    plt.show()\n",
        "\n",
        "  break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ejRPhG1HnFIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Variance Autoencoder"
      ],
      "metadata": {
        "id": "35sjafeBKiO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "        comet_experiment.set_epoch(epoch)\n",
        "\n",
        "        model.eval()\n",
        "        with comet_experiment.validate() as validat, torch.no_grad() as nograd:\n",
        "            for idx, batch in tqdm(enumerate(val_loader), desc=f\"VAL_{epoch}\"):\n",
        "                comet_experiment.set_step(idx + epoch * len(val_loader))\n",
        "\n",
        "                images = batch[\"image\"] / 255.0\n",
        "                # labels = batch[\"label\"]\n",
        "                images = images.cuda()\n",
        "                predictions, latents, mean, logvariance = model(images)\n",
        "                reconstruction_loss = reconstruction_loss_func(\n",
        "                    predictions, images)\n",
        "                kld_loss = kld_loss_func(mean, logvariance)\n",
        "                loss = reconstruction_loss + 0.01 * kld_loss\n",
        "\n",
        "                metric = structural_similarity_index_measure(\n",
        "                    predictions, images)\n",
        "\n",
        "                comet_experiment.log_metric(\n",
        "                    \"reconstruction_loss\", reconstruction_loss.item())\n",
        "                comet_experiment.log_metric(\"kld_loss\", kld_loss.item())\n",
        "                comet_experiment.log_metric(\"loss\", loss.item())\n",
        "                comet_experiment.log_metric(\"SSIM\", metric.item())\n",
        "\n",
        "            if idx < 4:\n",
        "                  # Rearrange tensors to HWC (Height-Width-Channel) format\n",
        "                  images = einops.rearrange(\n",
        "                      images, \"batch channel width height -> batch width height channel\"\n",
        "                  ).cpu().detach().numpy()\n",
        "\n",
        "                  predictions = einops.rearrange(\n",
        "                      predictions, \"batch channel width height -> batch width height channel\"\n",
        "                  ).cpu().detach().numpy()\n",
        "\n",
        "                  # Ensure pixel values are in range [0, 1] and convert to uint8\n",
        "                  images = np.clip(images, 0, 1)\n",
        "                  predictions = np.clip(predictions, 0, 1)\n",
        "\n",
        "                  images = (images * 255).astype(np.uint8)\n",
        "                  predictions = (predictions * 255).astype(np.uint8)\n",
        "\n",
        "                  # Log the first image in the batch as an example\n",
        "                  combined_image = np.hstack((images[0], predictions[0]))  # Side-by-side\n",
        "                  comet_experiment.log_image(combined_image, name=f\"comparison_{idx}\", step=num_epochs)\n",
        "\n",
        "        model.train()\n",
        "        with comet_experiment.train() as train:\n",
        "            for idx, batch in tqdm(enumerate(train_loader), desc=f\"TRAIN_{epoch}\"):\n",
        "                comet_experiment.set_step(idx + epoch * len(train_loader))\n",
        "\n",
        "                optimizer.zero_grad()  # MUST be called on every batch\n",
        "                images = batch[\"image\"] / 255.0\n",
        "                labels = batch[\"style\"]\n",
        "                images = images.cuda()\n",
        "                predictions, latents, mean, logvariance = model(images)\n",
        "                reconstruction_loss = reconstruction_loss_func(\n",
        "                    predictions, images)\n",
        "                kld_loss = kld_loss_func(mean, logvariance)\n",
        "                loss = reconstruction_loss + 0.01 * kld_loss\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                metric = structural_similarity_index_measure(\n",
        "                    predictions, images)\n",
        "\n",
        "                comet_experiment.log_metric(\n",
        "                    \"reconstruction_loss\", reconstruction_loss.item())\n",
        "                comet_experiment.log_metric(\"kld_loss\", kld_loss.item())\n",
        "                comet_experiment.log_metric(\"loss\", loss.item())\n",
        "                comet_experiment.log_metric(\"SSIM\", metric.item())\n",
        "\n",
        "                if not idx % 50:\n",
        "                    comet_experiment.log_histogram_3d(\n",
        "                        latents.detach().cpu(), \"latents\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ8a9TLyKlff",
        "outputId": "598c9236-6bd8-40fe-b3e8-481c210db572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "VAL_0: 323it [00:31, 10.16it/s]\n",
            "TRAIN_0: 2582it [09:35,  4.49it/s]\n",
            "VAL_1: 323it [00:32,  9.84it/s]\n",
            "TRAIN_1: 2582it [09:35,  4.49it/s]\n",
            "VAL_2: 323it [00:32,  9.80it/s]\n",
            "TRAIN_2: 2582it [09:35,  4.49it/s]\n",
            "VAL_3: 323it [00:32,  9.88it/s]\n",
            "TRAIN_3: 1974it [07:19,  4.29it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    model.eval()\n",
        "    with comet_experiment.test() as test, torch.no_grad() as nograd:\n",
        "        for idx, batch in tqdm(enumerate(val_loader), desc=f\"TEST_{num_epochs}\"):\n",
        "            comet_experiment.set_step(idx + num_epochs * len(val_loader))\n",
        "\n",
        "            images = batch[\"image\"] / 255.0\n",
        "            labels = batch[\"style\"]\n",
        "            images = images.cuda()\n",
        "            predictions, latents, mean, logvariance = model(images)\n",
        "            reconstruction_loss = reconstruction_loss_func(\n",
        "                predictions, images)\n",
        "            kld_loss = kld_loss_func(mean, logvariance)\n",
        "            loss = reconstruction_loss + 0.01 * kld_loss\n",
        "\n",
        "            metric = structural_similarity_index_measure(\n",
        "                predictions, images)\n",
        "\n",
        "            comet_experiment.log_metric(\n",
        "                \"reconstruction_loss\", reconstruction_loss.item())\n",
        "            comet_experiment.log_metric(\"kld_loss\", kld_loss.item())\n",
        "            comet_experiment.log_metric(\"loss\", loss.item())\n",
        "            comet_experiment.log_metric(\"SSIM\", metric.item())\n",
        "\n",
        "        if idx < 4:\n",
        "          try:\n",
        "              # Rearrange tensors to HWC (Height-Width-Channel) format\n",
        "              images = einops.rearrange(\n",
        "                  images, \"batch channel width height -> batch width height channel\"\n",
        "              ).cpu().detach().numpy()\n",
        "\n",
        "              predictions = einops.rearrange(\n",
        "                  predictions, \"batch channel width height -> batch width height channel\"\n",
        "              ).cpu().detach().numpy()\n",
        "\n",
        "              # Ensure pixel values are in range [0, 1] and convert to uint8\n",
        "              images = np.clip(images, 0, 1)\n",
        "              predictions = np.clip(predictions, 0, 1)\n",
        "\n",
        "              images = (images * 255).astype(np.uint8)\n",
        "              predictions = (predictions * 255).astype(np.uint8)\n",
        "\n",
        "              # Log the first image in the batch as an example\n",
        "              combined_image = np.hstack((images[0], predictions[0]))  # Side-by-side\n",
        "              comet_experiment.log_image(combined_image, name=f\"comparison_{idx}\", step=num_epochs)\n"
      ],
      "metadata": {
        "id": "hhP_bOD4Kw_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log model\n",
        "log_model(comet_experiment, model, model_name=\"AutoEncoder\")"
      ],
      "metadata": {
        "id": "Z80C_3Yaqamt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZL1gXNDsvz31"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}