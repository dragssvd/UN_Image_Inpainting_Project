{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_VAUkJmgEdX"
   },
   "source": [
    "# Klasyfikator klastrów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laxkoQ58ggfA"
   },
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8W9ttzHf99W",
    "outputId": "5693aedd-35e6-4ee5-a49b-28c13dd816ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: comet-ml in ./.venv/lib/python3.12/site-packages (3.47.3)\n",
      "Requirement already satisfied: everett<3.2.0,>=1.0.1 in ./.venv/lib/python3.12/site-packages (from everett[ini]<3.2.0,>=1.0.1->comet-ml) (3.1.0)\n",
      "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in ./.venv/lib/python3.12/site-packages (from comet-ml) (4.23.0)\n",
      "Requirement already satisfied: psutil>=5.6.3 in ./.venv/lib/python3.12/site-packages (from comet-ml) (6.1.0)\n",
      "Requirement already satisfied: python-box<7.0.0 in ./.venv/lib/python3.12/site-packages (from comet-ml) (6.1.0)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in ./.venv/lib/python3.12/site-packages (from comet-ml) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in ./.venv/lib/python3.12/site-packages (from comet-ml) (2.32.3)\n",
      "Requirement already satisfied: semantic-version>=2.8.0 in ./.venv/lib/python3.12/site-packages (from comet-ml) (2.10.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.1.0 in ./.venv/lib/python3.12/site-packages (from comet-ml) (2.18.0)\n",
      "Requirement already satisfied: simplejson in ./.venv/lib/python3.12/site-packages (from comet-ml) (3.19.3)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in ./.venv/lib/python3.12/site-packages (from comet-ml) (2.2.3)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in ./.venv/lib/python3.12/site-packages (from comet-ml) (1.16.0)\n",
      "Requirement already satisfied: wurlitzer>=1.0.2 in ./.venv/lib/python3.12/site-packages (from comet-ml) (3.1.1)\n",
      "Requirement already satisfied: dulwich!=0.20.33,>=0.20.6 in ./.venv/lib/python3.12/site-packages (from comet-ml) (0.22.6)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from comet-ml) (75.5.0)\n",
      "Requirement already satisfied: rich>=13.3.2 in ./.venv/lib/python3.12/site-packages (from comet-ml) (13.9.4)\n",
      "Requirement already satisfied: configobj in ./.venv/lib/python3.12/site-packages (from everett[ini]<3.2.0,>=1.0.1->comet-ml) (5.0.9)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.12/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.12/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.12/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.12/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.18.4->comet-ml) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.18.4->comet-ml) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.18.4->comet-ml) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich>=13.3.2->comet-ml) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich>=13.3.2->comet-ml) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet-ml) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install comet-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e45-0lqzgj9j"
   },
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45PCWipEgnBU"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# from google.colab import userdata\n",
    "# key = userdata.get('COMET_API_KEY')\n",
    "# os.environ['COMET_API_KEY'] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWG9YAAkgoJc",
    "outputId": "8f44db06-3666-462a-aeef-06af544ec0ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "def setup_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        # Set default tensor type for cuda\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        # Ensure we're using float32 on CPU\n",
    "        torch.set_default_dtype(torch.float64)\n",
    "    return device\n",
    "\n",
    "device = setup_device()\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s2k90WCgqQY"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_PJ_NkxgrlL",
    "outputId": "ae8347dc-f12b-4dc8-87d3-adddb446c5d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets) (4.67.0)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets) (3.11.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "5vSo6Jsrgs9s"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, load_from_disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qv7sMSzHjQjJ",
    "outputId": "64ffdebe-8aa1-439f-9d4c-7fc140e25336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPAINTING_AUTOENC.ipynb      dataset_clusterization.ipynb\n",
      "VGG16.py                      dataset_rename.ipynb\n",
      "VGG16_inpating.py             learn_model_per_cluster.ipynb\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m                   losowanie.ipynb\n",
      "cluster_clasification.ipynb   main.py\n",
      "\u001b[34mclustered_whole_ds_2\u001b[m\u001b[m          model1.pth\n",
      "create_mask.py                requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "uNu2rfoPgugO",
    "outputId": "1862995f-20a0-46c3-b79c-fe8d77f16ab8"
   },
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"clustered_whole_ds_2\").with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "vON2G2Cxg2H_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
       "        num_rows: 82600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
       "        num_rows: 10325\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
       "        num_rows: 10325\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:42<00:00, 1955.79 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1927.00 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1992.29 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:43<00:00, 1903.60 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 2011.37 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 2001.88 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:43<00:00, 1902.70 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1934.25 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1938.43 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:43<00:00, 1893.23 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 2013.11 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1986.23 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:41<00:00, 1980.82 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1991.71 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1915.48 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:44<00:00, 1868.53 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1728.30 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1755.98 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:47<00:00, 1755.62 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1817.76 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1772.74 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:44<00:00, 1837.30 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1825.20 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1929.16 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:43<00:00, 1889.95 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1896.87 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1863.51 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:43<00:00, 1913.15 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1934.61 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1904.64 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:42<00:00, 1956.88 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1828.59 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1880.45 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:43<00:00, 1905.62 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1868.72 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1919.81 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:42<00:00, 1924.09 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1801.46 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1883.24 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:44<00:00, 1866.40 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1788.39 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1840.92 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:44<00:00, 1851.97 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1914.30 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1919.51 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:43<00:00, 1882.64 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1916.55 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1884.78 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:44<00:00, 1845.39 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1763.00 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1803.61 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:44<00:00, 1867.26 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1816.80 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1898.42 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:44<00:00, 1876.15 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1936.98 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1875.35 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 82600/82600 [00:43<00:00, 1914.11 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1991.48 examples/s]\n",
      "Filter: 100%|██████████| 10325/10325 [00:05<00:00, 1941.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Create subsets for each cluster\n",
    "cluster_subsets = {}\n",
    "\n",
    "for cluster_id in range(0,20):\n",
    "    # Filter the dataset for the current cluster\n",
    "    print(int(cluster_id))\n",
    "    cluster_data = ds.filter(lambda example: example['cluster'] == cluster_id)\n",
    "    \n",
    "    # Add to the cluster_subsets dictionary\n",
    "    cluster_subsets[int(cluster_id)] = cluster_data\n",
    "\n",
    "# cluster_subsets now contains subsets for 20 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 2904\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 380\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 389\n",
      "    })\n",
      "}), 1: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 3044\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 354\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 383\n",
      "    })\n",
      "}), 2: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 4609\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 556\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 590\n",
      "    })\n",
      "}), 3: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 4882\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 597\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 607\n",
      "    })\n",
      "}), 4: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 4265\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 512\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 525\n",
      "    })\n",
      "}), 5: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 5019\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 672\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 588\n",
      "    })\n",
      "}), 6: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 3541\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 485\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 465\n",
      "    })\n",
      "}), 7: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 5985\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 770\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 741\n",
      "    })\n",
      "}), 8: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 5447\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 651\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 690\n",
      "    })\n",
      "}), 9: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 4014\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 505\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 494\n",
      "    })\n",
      "}), 10: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 3227\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 412\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 416\n",
      "    })\n",
      "}), 11: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 4019\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 491\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 491\n",
      "    })\n",
      "}), 12: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 4302\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 509\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 550\n",
      "    })\n",
      "}), 13: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 3738\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 473\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 457\n",
      "    })\n",
      "}), 14: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 6697\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 841\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 845\n",
      "    })\n",
      "}), 15: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 5320\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 677\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 686\n",
      "    })\n",
      "}), 16: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 2344\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 313\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 256\n",
      "    })\n",
      "}), 17: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 1311\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 155\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 178\n",
      "    })\n",
      "}), 18: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 5285\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 657\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 652\n",
      "    })\n",
      "}), 19: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 2647\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 315\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['title', 'artist', 'date', 'genre', 'style', 'description', 'filename', 'image', 'cluster'],\n",
      "        num_rows: 322\n",
      "    })\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "print(cluster_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2904/2904 [00:00<00:00, 42334.90 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 380/380 [00:00<00:00, 37016.01 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 389/389 [00:00<00:00, 39141.74 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3044/3044 [00:00<00:00, 47104.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 354/354 [00:00<00:00, 33992.30 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 383/383 [00:00<00:00, 36331.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4609/4609 [00:00<00:00, 45042.25 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 556/556 [00:00<00:00, 37786.52 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 590/590 [00:00<00:00, 38767.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4882/4882 [00:00<00:00, 40376.05 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 597/597 [00:00<00:00, 31597.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 607/607 [00:00<00:00, 33285.52 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4265/4265 [00:00<00:00, 42163.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 512/512 [00:00<00:00, 35344.29 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 525/525 [00:00<00:00, 37620.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5019/5019 [00:00<00:00, 50549.80 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 672/672 [00:00<00:00, 40878.50 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 588/588 [00:00<00:00, 39153.05 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3541/3541 [00:00<00:00, 51490.35 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 485/485 [00:00<00:00, 37547.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 465/465 [00:00<00:00, 40291.52 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5985/5985 [00:00<00:00, 49816.85 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 770/770 [00:00<00:00, 29758.62 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 741/741 [00:00<00:00, 41341.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5447/5447 [00:00<00:00, 40943.25 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 651/651 [00:00<00:00, 33626.33 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 690/690 [00:00<00:00, 38514.68 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4014/4014 [00:00<00:00, 53737.09 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 505/505 [00:00<00:00, 42529.18 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 494/494 [00:00<00:00, 41544.42 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3227/3227 [00:00<00:00, 41801.07 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 412/412 [00:00<00:00, 39406.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 416/416 [00:00<00:00, 33314.82 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4019/4019 [00:00<00:00, 47878.88 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 491/491 [00:00<00:00, 42870.30 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 491/491 [00:00<00:00, 35708.89 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4302/4302 [00:00<00:00, 45721.26 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 509/509 [00:00<00:00, 35198.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 550/550 [00:00<00:00, 37077.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3738/3738 [00:00<00:00, 51167.74 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 473/473 [00:00<00:00, 36088.07 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 457/457 [00:00<00:00, 34540.62 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6697/6697 [00:00<00:00, 48510.38 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 841/841 [00:00<00:00, 34653.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 845/845 [00:00<00:00, 36518.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5320/5320 [00:00<00:00, 46184.84 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 677/677 [00:00<00:00, 38461.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 686/686 [00:00<00:00, 42741.80 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2344/2344 [00:00<00:00, 36372.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 313/313 [00:00<00:00, 32530.90 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 256/256 [00:00<00:00, 27968.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1311/1311 [00:00<00:00, 40501.84 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 155/155 [00:00<00:00, 21328.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 178/178 [00:00<00:00, 27746.91 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5285/5285 [00:00<00:00, 48325.27 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 657/657 [00:00<00:00, 39693.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 652/652 [00:00<00:00, 39488.91 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2647/2647 [00:00<00:00, 45321.52 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 315/315 [00:00<00:00, 30979.31 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 322/322 [00:00<00:00, 35433.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "for cluster_id, subset in cluster_subsets.items():\n",
    "    subset.set_format(\"arrow\")\n",
    "    subset.save_to_disk(f\"./cluster_{cluster_id}_subset\")\n",
    "    subset.set_format(\"torch\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change if not colab\n",
    "num_workers = 12\n",
    "pin_memory = True\n",
    "batch_size = 32\n",
    "\n",
    "cluster_dataloaders = {}\n",
    "\n",
    "for cluster_id in range(0,20):\n",
    "    subset = cluster_subsets[cluster_id]\n",
    "    cluster_dataloaders[cluster_id] = {\n",
    "            \"train\": DataLoader(\n",
    "                subset[\"train\"],\n",
    "                batch_size=batch_size,\n",
    "                num_workers=num_workers,\n",
    "                shuffle=True,\n",
    "                pin_memory=pin_memory,\n",
    "            ),\n",
    "            \"test\": DataLoader(\n",
    "                subset[\"test\"],\n",
    "                batch_size=batch_size,\n",
    "                num_workers=num_workers,\n",
    "                shuffle=False,\n",
    "                pin_memory=pin_memory,\n",
    "            ),\n",
    "            \"valid\": DataLoader(\n",
    "                subset[\"valid\"],\n",
    "                batch_size=batch_size,\n",
    "                num_workers=num_workers,\n",
    "                shuffle=False,\n",
    "                pin_memory=pin_memory,\n",
    "            ),\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'train': <torch.utils.data.dataloader.DataLoader object at 0x34f6ffa70>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x34ef1fef0>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x169c60d70>}, 1: {'train': <torch.utils.data.dataloader.DataLoader object at 0x34f6ff3b0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x168432570>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x169e061e0>}, 2: {'train': <torch.utils.data.dataloader.DataLoader object at 0x169ef0980>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x169fbaf90>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x34f04d010>}, 3: {'train': <torch.utils.data.dataloader.DataLoader object at 0x34f04c200>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x34f04f650>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x34f04c590>}, 4: {'train': <torch.utils.data.dataloader.DataLoader object at 0x34f04ffb0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x34f04cf50>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x34f04f980>}, 5: {'train': <torch.utils.data.dataloader.DataLoader object at 0x34f04e8d0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x34f04f260>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x34f04c8c0>}, 6: {'train': <torch.utils.data.dataloader.DataLoader object at 0x34f04fec0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x34f04e990>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x34f04c770>}, 7: {'train': <torch.utils.data.dataloader.DataLoader object at 0x34f04f080>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x34f04cce0>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x34f04dfa0>}, 8: {'train': <torch.utils.data.dataloader.DataLoader object at 0x34f04fc20>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x34f04d1f0>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x168120b00>}, 9: {'train': <torch.utils.data.dataloader.DataLoader object at 0x169c6d700>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x4cca9b8c0>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x34f4cc230>}, 10: {'train': <torch.utils.data.dataloader.DataLoader object at 0x34f4cd0d0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x34f4cf830>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x34f4cf470>}, 11: {'train': <torch.utils.data.dataloader.DataLoader object at 0x34f4cd4c0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x34f4cff20>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x34f4ceae0>}, 12: {'train': <torch.utils.data.dataloader.DataLoader object at 0x34f4cf890>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x34f4cc8f0>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x34f4cc1d0>}, 13: {'train': <torch.utils.data.dataloader.DataLoader object at 0x34f4ccda0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x34f4cd5b0>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x34f4cf530>}, 14: {'train': <torch.utils.data.dataloader.DataLoader object at 0x34f4ce7b0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x34f4ce4e0>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x34f4cd310>}, 15: {'train': <torch.utils.data.dataloader.DataLoader object at 0x34f4ce7e0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x34f4cd7c0>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x34f4ced50>}, 16: {'train': <torch.utils.data.dataloader.DataLoader object at 0x34f4ce900>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x34f4ce750>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x34f4cd490>}, 17: {'train': <torch.utils.data.dataloader.DataLoader object at 0x34f4cf8f0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x34f4cde50>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x34f4cd3a0>}, 18: {'train': <torch.utils.data.dataloader.DataLoader object at 0x34f4cf1a0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x34f4cdd90>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x34f4cfc50>}, 19: {'train': <torch.utils.data.dataloader.DataLoader object at 0x34f4cfce0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x34f4ceb70>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x34f4ce1b0>}}\n"
     ]
    }
   ],
   "source": [
    "print(cluster_dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_eSbu9vjozd"
   },
   "source": [
    "## AUTOENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsIS8DP0imCW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rb/07333lmj29n9vlhvb4phkvpm0000gn/T/ipykernel_41190/743672055.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(PATH)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG16Autoencoder(\n",
       "  (encoder): VGG16EncoderWithSkipConnections(\n",
       "    (block1): Sequential(\n",
       "      (0): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (block3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (block4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (block5): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (block6): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder): VGG16Decoder(\n",
       "    (block6): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (block5): Sequential(\n",
       "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (block4): Sequential(\n",
       "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "    )\n",
       "    (block1): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (5): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"model1.pth\"\n",
    "# model = VGG16Autoencoder()\n",
    "new_model = torch.load(PATH)\n",
    "new_model.eval()\n",
    "new_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.feature_extractor = models.vgg16(pretrained=True).features.eval()\n",
    "        self.layers = ['4', '9', '16']\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, predicted, targets):\n",
    "        pred_features = self.extract_features(predicted)\n",
    "        target_features = self.extract_features(targets)\n",
    "\n",
    "        loss = 0.0\n",
    "        for pred_feat, target_feat in zip(pred_features, target_features):\n",
    "            loss += nn.functional.mse_loss(pred_feat, target_feat)\n",
    "        return loss\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        features = []\n",
    "        for name, layer in self.feature_extractor._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.layers:\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.perceptual_loss = PerceptualLoss()\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, predicted, targets, masks):\n",
    "        loss_weights = torch.where(masks == 0.0, torch.tensor(1.0), torch.tensor(10.0))\n",
    "        loss_weights = loss_weights.unsqueeze(1)  # batch 1 h w\n",
    "        loss_weights = loss_weights.to(predicted.device)\n",
    "\n",
    "\n",
    "        mse = self.mse_loss(predicted, targets) * loss_weights\n",
    "        mse = mse.mean()\n",
    "\n",
    "        perceptual = self.perceptual_loss(predicted, targets)\n",
    "\n",
    "        return self.alpha * mse + (1 - self.alpha) * perceptual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baban/Documents/SIUM2/AUTOENC/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/baban/Documents/SIUM2/AUTOENC/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /Users/baban/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:1344\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1344\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1336\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1336\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1382\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1381\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1382\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1091\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1091\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1094\u001b[0m \n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1035\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m-> 1035\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1477\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1475\u001b[0m     server_hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[0;32m-> 1477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    450\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    452\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1041\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1319\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1319\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcreate_mask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_scaled_blob\n\u001b[1;32m      5\u001b[0m LR \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3e-5\u001b[39m\n\u001b[0;32m----> 7\u001b[0m loss_func \u001b[38;5;241m=\u001b[39m \u001b[43mCombinedLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLR)\n",
      "Cell \u001b[0;32mIn[52], line 32\u001b[0m, in \u001b[0;36mCombinedLoss.__init__\u001b[0;34m(self, alpha)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28msuper\u001b[39m(CombinedLoss, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperceptual_loss \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptualLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmse_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m=\u001b[39m alpha\n",
      "Cell \u001b[0;32mIn[52], line 6\u001b[0m, in \u001b[0;36mPerceptualLoss.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28msuper\u001b[39m(PerceptualLoss, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvgg16\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m9\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m16\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/Documents/SIUM2/AUTOENC/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs\u001b[38;5;241m.\u001b[39mkeys()),\u001b[38;5;250m \u001b[39mseparate_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as positional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/SIUM2/AUTOENC/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[1;32m    226\u001b[0m     kwargs[weights_param] \u001b[38;5;241m=\u001b[39m default_weights_arg\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/SIUM2/AUTOENC/.venv/lib/python3.12/site-packages/torchvision/models/vgg.py:433\u001b[0m, in \u001b[0;36mvgg16\u001b[0;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"VGG-16 from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m    :members:\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    431\u001b[0m weights \u001b[38;5;241m=\u001b[39m VGG16_Weights\u001b[38;5;241m.\u001b[39mverify(weights)\n\u001b[0;32m--> 433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_vgg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/SIUM2/AUTOENC/.venv/lib/python3.12/site-packages/torchvision/models/vgg.py:105\u001b[0m, in \u001b[0;36m_vgg\u001b[0;34m(cfg, batch_norm, weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m model \u001b[38;5;241m=\u001b[39m VGG(make_layers(cfgs[cfg], batch_norm\u001b[38;5;241m=\u001b[39mbatch_norm), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Documents/SIUM2/AUTOENC/.venv/lib/python3.12/site-packages/torchvision/models/_api.py:90\u001b[0m, in \u001b[0;36mWeightsEnum.get_state_dict\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_state_dict_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/SIUM2/AUTOENC/.venv/lib/python3.12/site-packages/torch/hub.py:867\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[0m\n\u001b[1;32m    865\u001b[0m         r \u001b[38;5;241m=\u001b[39m HASH_REGEX\u001b[38;5;241m.\u001b[39msearch(filename)  \u001b[38;5;66;03m# r is Optional[Match[str]]\u001b[39;00m\n\u001b[1;32m    866\u001b[0m         hash_prefix \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m     \u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n",
      "File \u001b[0;32m~/Documents/SIUM2/AUTOENC/.venv/lib/python3.12/site-packages/torch/hub.py:708\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    706\u001b[0m file_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    707\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.hub\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m--> 708\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m meta \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39minfo()\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(meta, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:515\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    512\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    514\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 515\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    518\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:532\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    531\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 532\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    533\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:1392\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1393\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:1347\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1344\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[1;32m   1345\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1348\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)>"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "from tqdm import tqdm \n",
    "from create_mask import generate_scaled_blob\n",
    "\n",
    "LR = 3e-5\n",
    "\n",
    "loss_func = CombinedLoss(alpha=0.7).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rb/07333lmj29n9vlhvb4phkvpm0000gn/T/ipykernel_41190/681371507.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  new_model = torch.load(PATH)\n",
      "TRAIN_0: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRAIN_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 22\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero out gradients before each batch\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     images \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m     25\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Assuming you still have this in your dataset\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "from create_mask import generate_scaled_blob\n",
    "\n",
    "\n",
    "for cluster in range(0,20):\n",
    "    \n",
    "    PATH = \"model1.pth\"\n",
    "    # model = VGG16Autoencoder()\n",
    "    new_model = torch.load(PATH)\n",
    "    new_model.eval()\n",
    "    new_model.to(device)\n",
    "    \n",
    "    train_loader, test_loader, val_loader = cluster_dataloaders[cluster]\n",
    "\n",
    "\n",
    "    # Train and validate\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "\n",
    "        for idx, batch in tqdm(enumerate(train_loader), desc=f\"TRAIN_{epoch}\"):\n",
    "            optimizer.zero_grad()  # Zero out gradients before each batch\n",
    "\n",
    "            images = batch[\"image\"].float() / 255.0\n",
    "            labels = batch[\"style\"]  # Assuming you still have this in your dataset\n",
    "\n",
    "            # masks with corrupted part of image\n",
    "            masks = generate_scaled_blob(images.shape, mask_percentage=(1 / 16) * 100).float() / 255.0\n",
    "\n",
    "            # apply mask\n",
    "            images_with_mask = images * (1 - masks.unsqueeze(1))\n",
    "\n",
    "            # add mask as 4th channel\n",
    "            images_with_mask = torch.cat((images_with_mask, masks.unsqueeze(1)), dim=1)  # batch 4 imgsize imgsize\n",
    "\n",
    "            inputs = images_with_mask.to(device)\n",
    "            targets = images.to(device)\n",
    "\n",
    "            reconstructed_image = model(inputs)\n",
    "            loss = loss_func(reconstructed_image, targets, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "\n",
    "        for idx, batch in tqdm(enumerate(val_loader), desc=f\"VAL_{epoch}\"):\n",
    "\n",
    "            images = batch[\"image\"].float() / 255.0\n",
    "\n",
    "            # masks with corrupted part of image\n",
    "            masks = generate_scaled_blob(images.shape, mask_percentage=(1 / 16) * 100).float() / 255.0\n",
    "\n",
    "            #apply mask\n",
    "            images_with_mask = images * (1- masks.unsqueeze(1))\n",
    "\n",
    "            #add mask as 4th channel\n",
    "            images_with_mask = torch.cat((images_with_mask, masks.unsqueeze(1)), dim=1) # batch 4 imgsize imgsize\n",
    "\n",
    "            inputs = images_with_mask.to(device)\n",
    "            targets = images.to(device)\n",
    "\n",
    "            # Forward pass through the model\n",
    "            # Autoencoder directly outputs the reconstructed image\n",
    "            reconstructed_image = model(inputs)\n",
    "\n",
    "            loss = loss_func(reconstructed_image, targets, masks)\n",
    "\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Start test\")\n",
    "    # Begin the test phase\n",
    "    for idx, batch in tqdm(enumerate(test_loader), desc=f\"TEST_{num_epochs}\"):\n",
    "\n",
    "        # Load and preprocess images\n",
    "        images = batch[\"image\"].float() / 255.0  # Assuming images are in [0, 255], normalize to [0, 1]\n",
    "        labels = batch[\"style\"]  # Assuming you still have this in your dataset (though not used here)\n",
    "\n",
    "        # masks with corrupted part of image\n",
    "        masks = generate_scaled_blob(images.shape, mask_percentage=(1 / 16) * 100).float() / 255.0\n",
    "\n",
    "        # apply mask\n",
    "        images_with_mask = images * (1 - masks.unsqueeze(1))\n",
    "\n",
    "        # add mask as 4th channel\n",
    "        images_with_mask = torch.cat((images_with_mask, masks.unsqueeze(1)), dim=1)  # batch 4 imgsize imgsize\n",
    "        \n",
    "        inputs = images_with_mask.to(device)\n",
    "        targets = images.to(device)\n",
    "        \n",
    "        reconstructed_image = model(inputs)\n",
    "\n",
    "        loss = loss_func(reconstructed_image, targets, masks)\n",
    "        \n",
    "        \n",
    "        print(\"Save model dict\")\n",
    "        torch.save(model.state_dict(), f\"./cluster_{cluster_id}_submodel_dict.pth\")\n",
    "        print(\"Save model\")\n",
    "        torch.save(model, f\"./cluster_{cluster_id}_submodel.pth\")\n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
